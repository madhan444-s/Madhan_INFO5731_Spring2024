{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhan444-s/Madhan_INFO5731_Spring2024/blob/main/Dadi_Madhan_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776ea918-2111-427a-e9b0-1589bb866076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 reviews collected and saved to 'movie_reviews.csv'\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to get movie reviews from IMDb\n",
        "def getMovieReviews(movieId, numReviews=1000):\n",
        "    baseUrl = f'https://www.imdb.com/title/{movieId}/reviews'\n",
        "\n",
        "    reviewsData = []\n",
        "    pageNumber = 1\n",
        "\n",
        "    # Loop until the desired number of reviews is collected\n",
        "    while len(reviewsData) < numReviews:\n",
        "        url = f'{baseUrl}?spoiler=hide&sort=helpfulnessScore&dir=desc&ratingFilter=0&page={pageNumber}'\n",
        "        response = requests.get(url)        # Send a request to the IMDb website\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            reviewElements = soup.find_all('div', class_='lister-item-content')\n",
        "\n",
        "            if not reviewElements:\n",
        "                break\n",
        "\n",
        "            # Loop through each review element and extract author and review text\n",
        "            for review in reviewElements:\n",
        "                author = review.find('span', class_='display-name-link').text.strip()\n",
        "                review = review.find('div', class_='text show-more__control').text.strip()\n",
        "\n",
        "                reviewsData.append({\n",
        "                    'Author': author,\n",
        "                    'Review': review,\n",
        "                })\n",
        "\n",
        "            pageNumber += 1\n",
        "        else:\n",
        "            print(f\"Failed to fetch reviews. Status code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "    return reviewsData[:numReviews]\n",
        "\n",
        "# Function to save reviews data to a CSV file\n",
        "def saveReviewsDataToCSV(reviewsData, csvFileName='movie_reviews.csv'):\n",
        "    with open(csvFileName, mode='w', encoding='utf-8', newline='') as file:\n",
        "        fieldnames = ['Author', 'Review']\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for review_data in reviewsData:\n",
        "            writer.writerow({\n",
        "                'Author': review_data['Author'],\n",
        "                'Review': review_data['Review'],\n",
        "            })\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    movieId = \"tt15398776\"  # IMDb ID for \"Oppenheimer\" - Change this to the IMDb ID of your movie\n",
        "    numReviewsNeeded = 1000\n",
        "\n",
        "    reviewsData = getMovieReviews(movieId, numReviewsNeeded)\n",
        "\n",
        "    if reviewsData:\n",
        "        saveReviewsDataToCSV(reviewsData)\n",
        "        print(f\"{numReviewsNeeded} reviews collected and saved to 'movie_reviews.csv'\")\n",
        "    else:\n",
        "        print(\"Unable to collect reviews.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31319c71-c1b4-4bde-e168-751a20800bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to 'movie_reviews.csv'\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "# Write code for each of the sub parts with proper comments.\n",
        "# !pip install nltk\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def cleanText(text):\n",
        "    # Remove noise (special characters and punctuations)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d', '', text)\n",
        "\n",
        "    # Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def removeStopWords(text):\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    filteredText = ' '.join([word for word in words if word.lower() not in stopWords])\n",
        "    return filteredText\n",
        "\n",
        "def applyStemming(text):\n",
        "    ps = PorterStemmer()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stemmedText = ' '.join([ps.stem(word) for word in words])\n",
        "    return stemmedText\n",
        "\n",
        "def applyLemmatization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lemmatizedText = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "    return lemmatizedText\n",
        "\n",
        "def cleanAndSaveCsv(inputCsv, outputCsv):\n",
        "    with open(inputCsv, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        rows = list(reader)\n",
        "\n",
        "    for row in rows:\n",
        "        # Clean the 'Review' column\n",
        "        cleanedText = row['Review']\n",
        "        cleanedText = cleanText(cleanedText)\n",
        "        cleanedText = removeStopWords(cleanedText)\n",
        "        cleanedText = applyStemming(cleanedText)\n",
        "        cleanedText = applyLemmatization(cleanedText)\n",
        "        row['Cleaned Text'] = cleanedText\n",
        "\n",
        "    fieldnames = reader.fieldnames + ['Cleaned Text']\n",
        "\n",
        "    with open(outputCsv, 'w', encoding='utf-8', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inputCsv = 'movie_reviews.csv'\n",
        "    outputCsv = 'movie_reviews.csv'\n",
        "\n",
        "    cleanAndSaveCsv(inputCsv, outputCsv)\n",
        "    print(f\"Cleaned data saved to '{outputCsv}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151692bd-0ecc-4c82-f6f8-670000e52554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 1/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5104\n",
            "Total Verbs: 2029\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "you\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('you', 'nsubj', 'wit'), ('ll', 'aux', 'wit'), ('wit', 'nsubj', 'absolut'), ('brain', 'compound', 'switch'), ('fulli', 'compound', 'switch'), ('switch', 'compound', 'watch'), ('watch', 'dobj', 'wit'), ('oppenheim', 'nsubj', 'easili'), ('could', 'aux', 'easili'), ('easili', 'ccomp', 'wit')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "PERSON: 476\n",
            "ORG: 176\n",
            "NORP: 102\n",
            "GPE: 158\n",
            "ORDINAL: 73\n",
            "FAC: 10\n",
            "LOC: 4\n",
            "DATE: 57\n",
            "EVENT: 11\n",
            "CARDINAL: 154\n",
            "WORK_OF_ART: 8\n",
            "TIME: 44\n",
            "PRODUCT: 13\n",
            "QUANTITY: 3\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 2/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5066\n",
            "Total Verbs: 1961\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "act portray oppenheim\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('act', 'compound', 'oppenheim'), ('portray', 'compound', 'oppenheim'), ('oppenheim', 'nsubj', 'polit'), ('polit', 'nsubj', 'see'), ('scapegoat', 'dobj', 'polit'), ('done', 'acl', 'scapegoat'), ('well', 'advmod', 'see'), ('see', 'ROOT', 'see'), ('actor', 'compound', 'shinehowev'), ('shinehowev', 'compound', 'flaw')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "PERSON: 514\n",
            "ORG: 174\n",
            "ORDINAL: 81\n",
            "GPE: 171\n",
            "QUANTITY: 4\n",
            "DATE: 61\n",
            "TIME: 47\n",
            "PRODUCT: 15\n",
            "FAC: 11\n",
            "CARDINAL: 153\n",
            "NORP: 100\n",
            "EVENT: 10\n",
            "LOC: 3\n",
            "WORK_OF_ART: 6\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 3/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5089\n",
            "Total Verbs: 2028\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "inematographi sound design score\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('inematographi', 'amod', 'score'), ('sound', 'amod', 'score'), ('design', 'compound', 'score'), ('score', 'nsubj', 'stand'), ('except', 'prep', 'score'), ('everi', 'amod', 'act'), ('act', 'nsubj', 'perform'), ('perform', 'pcomp', 'except'), ('perfect', 'nmod', 'downey'), ('cillian', 'compound', 'murphi')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "PERSON: 480\n",
            "NORP: 103\n",
            "DATE: 58\n",
            "CARDINAL: 154\n",
            "ORG: 175\n",
            "GPE: 156\n",
            "ORDINAL: 72\n",
            "FAC: 10\n",
            "LOC: 4\n",
            "EVENT: 11\n",
            "WORK_OF_ART: 8\n",
            "TIME: 44\n",
            "PRODUCT: 13\n",
            "QUANTITY: 3\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 4/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5072\n",
            "Total Verbs: 1959\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "best oppenheim\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('uss', 'compound', 'arguabl'), ('arguabl', 'advmod', 'cast'), ('best', 'amod', 'oppenheim'), ('oppenheim', 'nsubj', 'cast'), ('cast', 'nsubj', 'bring'), ('bring', 'ROOT', 'bring'), ('downey', 'dobj', 'bring'), ('jr', 'nmod', 'person'), ('perfectli', 'compound', 'portray'), ('portray', 'nmod', 'downfal')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "GPE: 172\n",
            "ORG: 175\n",
            "CARDINAL: 154\n",
            "ORDINAL: 82\n",
            "PERSON: 513\n",
            "QUANTITY: 4\n",
            "DATE: 61\n",
            "TIME: 47\n",
            "PRODUCT: 15\n",
            "FAC: 11\n",
            "NORP: 100\n",
            "EVENT: 10\n",
            "LOC: 3\n",
            "WORK_OF_ART: 6\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 5/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5086\n",
            "Total Verbs: 2027\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            " jennif\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[(' ', 'dep', 'jennif'), ('jennif', 'ROOT', 'jennif'), ('lame', 'compound', 'edit'), ('edit', 'nsubj', 'excel'), ('excel', 'ccomp', 'hope'), ('vital', 'amod', 'hoytema'), ('lastli', 'nmod', 'cinematograph'), ('cinematograph', 'nmod', 'hoytema'), ('regular', 'amod', 'hoytema'), ('nolan', 'compound', 'collabor')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "PERSON: 479\n",
            "GPE: 157\n",
            "NORP: 103\n",
            "DATE: 58\n",
            "CARDINAL: 153\n",
            "ORG: 175\n",
            "ORDINAL: 72\n",
            "FAC: 10\n",
            "LOC: 4\n",
            "EVENT: 11\n",
            "WORK_OF_ART: 8\n",
            "TIME: 44\n",
            "PRODUCT: 13\n",
            "QUANTITY: 3\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 6/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5073\n",
            "Total Verbs: 1962\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "c masterpiec\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('c', 'compound', 'masterpiec'), ('masterpiec', 'nsubj', 'haunt'), ('haunt', 'advcl', 'make'), ('score', 'compound', 'göransson'), ('ludwig', 'compound', 'göransson'), ('göransson', 'dobj', 'haunt'), ('one', 'nummod', 'film'), ('film', 'npadvmod', 'profound'), ('profound', 'amod', 'act'), ('highlight', 'compound', 'score')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "CARDINAL: 154\n",
            "PERSON: 514\n",
            "GPE: 172\n",
            "ORG: 175\n",
            "ORDINAL: 82\n",
            "QUANTITY: 4\n",
            "DATE: 61\n",
            "TIME: 47\n",
            "PRODUCT: 15\n",
            "FAC: 11\n",
            "NORP: 100\n",
            "EVENT: 10\n",
            "LOC: 3\n",
            "WORK_OF_ART: 6\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 7/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5094\n",
            "Total Verbs: 2022\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "best scene\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[(' ', 'dep', ' '), ('shine', 'ccomp', 'allow'), ('best', 'amod', 'scene'), ('scene', 'dobj', 'shine'), ('secur', 'dep', 'shine'), ('hearingchristoph', 'compound', 'deliv'), ('nolan', 'compound', 'deliv'), ('deliv', 'nsubj', 'perform'), ('near', 'prep', 'deliv'), ('perfect', 'amod', 'stori')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "CARDINAL: 153\n",
            "PERSON: 479\n",
            "GPE: 156\n",
            "NORP: 103\n",
            "DATE: 58\n",
            "ORG: 174\n",
            "ORDINAL: 72\n",
            "FAC: 10\n",
            "LOC: 4\n",
            "EVENT: 11\n",
            "WORK_OF_ART: 8\n",
            "TIME: 44\n",
            "PRODUCT: 13\n",
            "QUANTITY: 3\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 8/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5062\n",
            "Total Verbs: 1966\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "ridley scott\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('ridley', 'compound', 'scott'), ('scott', 'nsubj', 'return'), ('return', 'ROOT', 'return'), ('good', 'amod', 'tension'), ('old', 'amod', 'tension'), ('fashion', 'compound', 'nocgi'), ('nocgi', 'compound', 'drama'), ('drama', 'compound', 'tension'), ('tension', 'dobj', 'return'), ('come', 'advcl', 'return')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "CARDINAL: 153\n",
            "PERSON: 513\n",
            "ORG: 176\n",
            "GPE: 173\n",
            "ORDINAL: 82\n",
            "QUANTITY: 4\n",
            "DATE: 61\n",
            "TIME: 47\n",
            "PRODUCT: 15\n",
            "FAC: 11\n",
            "NORP: 99\n",
            "EVENT: 10\n",
            "LOC: 3\n",
            "WORK_OF_ART: 6\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 9/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5109\n",
            "Total Verbs: 2022\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "oppenheim struggl power creat lack power\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('oppenheim', 'compound', 'struggl'), ('struggl', 'compound', 'power'), ('power', 'compound', 'creat'), ('creat', 'compound', 'lack'), ('lack', 'compound', 'power'), ('power', 'nsubj', 'use'), ('use', 'compound', 'film'), ('favorit', 'compound', 'line'), ('line', 'compound', 'film'), ('film', 'nsubj', 'aim')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "PERSON: 479\n",
            "CARDINAL: 155\n",
            "NORP: 103\n",
            "GPE: 156\n",
            "DATE: 58\n",
            "ORG: 173\n",
            "ORDINAL: 72\n",
            "FAC: 10\n",
            "LOC: 4\n",
            "EVENT: 11\n",
            "WORK_OF_ART: 8\n",
            "TIME: 43\n",
            "PRODUCT: 13\n",
            "QUANTITY: 3\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 10/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5067\n",
            "Total Verbs: 1974\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "craft beauti ador\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('hi', 'intj', 'enjoy'), ('craft', 'compound', 'ador'), ('beauti', 'compound', 'ador'), ('ador', 'nsubj', 'enjoy'), ('enjoy', 'ROOT', 'enjoy'), ('whole', 'dobj', 'enjoy'), ('three', 'nummod', 'hour'), ('hour', 'compound', 'ea'), ('ea', 'punct', 'enjoy'), ('delight', 'amod', 'attempt')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "TIME: 48\n",
            "ORG: 176\n",
            "NORP: 100\n",
            "PERSON: 505\n",
            "CARDINAL: 153\n",
            "GPE: 173\n",
            "ORDINAL: 82\n",
            "QUANTITY: 4\n",
            "DATE: 61\n",
            "PRODUCT: 15\n",
            "FAC: 11\n",
            "EVENT: 10\n",
            "LOC: 3\n",
            "WORK_OF_ART: 6\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 11/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 5100\n",
            "Total Verbs: 2014\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "i\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('i', 'nsubj', 'pash'), ('pash', 'ccomp', 'see'), ('tom', 'compound', 'conti'), ('conti', 'compound', 'safdi'), ('albert', 'compound', 'safdi'), ('einstein', 'compound', 'benni'), ('benni', 'compound', 'safdi'), ('safdi', 'dobj', 'pash'), ('dane', 'advmod', 'krumholtz'), ('dehaan', 'compound', 'hartnett')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "PERSON: 485\n",
            "ORG: 173\n",
            "CARDINAL: 153\n",
            "NORP: 103\n",
            "GPE: 155\n",
            "DATE: 57\n",
            "ORDINAL: 72\n",
            "FAC: 10\n",
            "LOC: 4\n",
            "EVENT: 11\n",
            "WORK_OF_ART: 8\n",
            "TIME: 43\n",
            "PRODUCT: 13\n",
            "QUANTITY: 3\n",
            "\n",
            "--------------------------------------------------\n",
            "Processing chunk 12/12...\n",
            "POS Tagging Results:\n",
            "Total Nouns: 2281\n",
            "Total Verbs: 875\n",
            "Total Adjectives: 0\n",
            "Total Adverbs: 0\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "hollywood miss mark time\n",
            "\n",
            "Explanation:\n",
            "Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('od', 'prep', 'expect'), ('hollywood', 'compound', 'miss'), ('miss', 'compound', 'mark'), ('mark', 'compound', 'time'), ('time', 'pobj', 'od'), ('expect', 'ROOT', 'expect'), ('one', 'nummod', 'truth'), ('agre', 'compound', 'truth'), ('truth', 'nsubj', 'everywher'), ('today', 'npadvmod', 'everywher')]\n",
            "\n",
            "Explanation:\n",
            "Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\n",
            "\n",
            "Named Entity Recognition:\n",
            "\n",
            "Entity Counts:\n",
            "PERSON: 242\n",
            "CARDINAL: 71\n",
            "DATE: 29\n",
            "GPE: 81\n",
            "ORG: 78\n",
            "TIME: 22\n",
            "NORP: 44\n",
            "ORDINAL: 38\n",
            "QUANTITY: 2\n",
            "PRODUCT: 7\n",
            "FAC: 5\n",
            "EVENT: 4\n",
            "LOC: 1\n",
            "WORK_OF_ART: 2\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import csv\n",
        "import spacy\n",
        "\n",
        "# Download the English model for spaCy\n",
        "# Make sure to run this command before executing the script for the first time:\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def posTagging(text):\n",
        "    doc = nlp(text)\n",
        "    posTags = [(token.text, token.pos_) for token in doc]\n",
        "    return posTags\n",
        "\n",
        "def constituencyParsing(text):\n",
        "    doc = nlp(text)\n",
        "    nounChunks = [chunk.text for chunk in doc.noun_chunks]\n",
        "    return nounChunks\n",
        "\n",
        "def dependencyParsing(text):\n",
        "    doc = nlp(text)\n",
        "    dependencyTree = [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "    return dependencyTree\n",
        "\n",
        "def namedEntityRecognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the cleaned text from the CSV file\n",
        "    with open('movie_reviews.csv', 'r', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        cleanedText = ' '.join([row['Cleaned Text'] for row in reader])\n",
        "\n",
        "    # Split the text into chunks of 100,000 characters\n",
        "    chunkSize = 100000\n",
        "    chunks = [cleanedText[i:i+chunkSize] for i in range(0, len(cleanedText), chunkSize)]\n",
        "\n",
        "    for idx, chunk in enumerate(chunks, start=1):\n",
        "        print(f\"Processing chunk {idx}/{len(chunks)}...\")\n",
        "\n",
        "        # (1) Parts of Speech (POS) Tagging\n",
        "        posTags = posTagging(chunk)\n",
        "        nounCount = len([word for word, pos in posTags if pos.startswith('N')])\n",
        "        verbCount = len([word for word, pos in posTags if pos.startswith('V')])\n",
        "        adjCount = len([word for word, pos in posTags if pos.startswith('J')])\n",
        "        advCount = len([word for word, pos in posTags if pos.startswith('R')])\n",
        "\n",
        "        print(\"POS Tagging Results:\")\n",
        "        print(f\"Total Nouns: {nounCount}\")\n",
        "        print(f\"Total Verbs: {verbCount}\")\n",
        "        print(f\"Total Adjectives: {adjCount}\")\n",
        "        print(f\"Total Adverbs: {advCount}\\n\")\n",
        "\n",
        "        # (2) Constituency Parsing\n",
        "        constituencyChunks = constituencyParsing(chunk)\n",
        "        print(\"Constituency Parsing Tree:\")\n",
        "        print(constituencyChunks[0])  # Print only the first sentence for example\n",
        "        print(\"\\nExplanation:\")\n",
        "        print(\"Constituency parsing represents the sentence structure in terms of noun chunks. Each chunk is a syntactic unit.\")\n",
        "\n",
        "        # (3) Dependency Parsing\n",
        "        dependencyTree = dependencyParsing(chunk)\n",
        "        print(\"\\nDependency Parsing Tree:\")\n",
        "        print(dependencyTree[:10])  # Print only the first 10 tokens for example\n",
        "        print(\"\\nExplanation:\")\n",
        "        print(\"Dependency parsing represents the grammatical structure of a sentence in terms of the relationships between words. Each tuple (word, dependency label, head word) describes a grammatical relationship.\")\n",
        "\n",
        "        # (4) Named Entity Recognition\n",
        "        namedEntities = namedEntityRecognition(chunk)\n",
        "        print(\"\\nNamed Entity Recognition:\")\n",
        "        entityCounts = {}\n",
        "        for entity, label in namedEntities:\n",
        "            #print(f\"{entity} - {label}\")\n",
        "            entityCounts[label] = entityCounts.get(label, 0) + 1\n",
        "\n",
        "        print(\"\\nEntity Counts:\")\n",
        "        for label, count in entityCounts.items():\n",
        "            print(f\"{label}: {count}\")\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "\n",
        "# Doing this assignment was kind of tough but interesting.\n",
        "# I learned how to get text from the internet, clean it up, and understand what the words mean.\n",
        "# Getting the text from websites or apps had some tricky parts, like dealing with changes on websites or limits on APIs.\n",
        "# Cleaning the text was like removing extra stuff and making it simpler to understand.\n",
        "# Figuring out what each word does in a sentence was another challenge, but it helped me see how words work together.\n",
        "# The assignment was like solving real-world problems with data from the internet, which made it cool.\n",
        "# I used special tools like BeautifulSoup and NLTK to make things easier.\n",
        "# The time they gave us to finish the assignment was okay if you know a bit about getting data from the web and some basics about words.\n",
        "# If you're new to this, some parts might take a bit longer. Overall, it was a good learning experience about words and data on the internet."
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}