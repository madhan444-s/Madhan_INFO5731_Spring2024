{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abffede-5dc8-42b7-be80-0bdb6c004d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating MultinomialNB:\n",
            "Mean cross-validation accuracy: 0.7821528126856464\n",
            "Validation Metrics:\n",
            "Accuracy: 0.7947976878612717\n",
            "Precision: 0.777490297542044\n",
            "Recall: 0.8429172510518934\n",
            "F1 Score: 0.8088829071332435\n",
            "--------------------------------------\n",
            "Evaluating SVM:\n",
            "Mean cross-validation accuracy: 0.7389859055627003\n",
            "Validation Metrics:\n",
            "Accuracy: 0.7557803468208093\n",
            "Precision: 0.7394636015325671\n",
            "Recall: 0.8120617110799438\n",
            "F1 Score: 0.7740641711229947\n",
            "--------------------------------------\n",
            "Evaluating KNN:\n",
            "Mean cross-validation accuracy: 0.5706295167155195\n",
            "Validation Metrics:\n",
            "Accuracy: 0.6163294797687862\n",
            "Precision: 0.6226415094339622\n",
            "Recall: 0.6479663394109397\n",
            "F1 Score: 0.6350515463917525\n",
            "--------------------------------------\n",
            "Evaluating DecisionTree:\n",
            "Mean cross-validation accuracy: 0.6221091388618694\n",
            "Validation Metrics:\n",
            "Accuracy: 0.6517341040462428\n",
            "Precision: 0.6460176991150443\n",
            "Recall: 0.7166900420757363\n",
            "F1 Score: 0.6795212765957447\n",
            "--------------------------------------\n",
            "Evaluating RandomForest:\n",
            "Mean cross-validation accuracy: 0.7232656138816171\n",
            "Validation Metrics:\n",
            "Accuracy: 0.7319364161849711\n",
            "Precision: 0.7055288461538461\n",
            "Recall: 0.82328190743338\n",
            "F1 Score: 0.7598705501618123\n",
            "--------------------------------------\n",
            "Evaluating XGBoost:\n",
            "Mean cross-validation accuracy: 0.7118911614364706\n",
            "Validation Metrics:\n",
            "Accuracy: 0.7398843930635838\n",
            "Precision: 0.7129071170084439\n",
            "Recall: 0.8288920056100981\n",
            "F1 Score: 0.7665369649805446\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load data\n",
        "def load_data(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    labels = []\n",
        "    texts = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split(' ', 1)  # Split only once at the first space\n",
        "        labels.append(int(parts[0]))\n",
        "        texts.append(parts[1])\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_data('stsa-train.txt')\n",
        "test_texts, test_labels = load_data('stsa-test.txt')\n",
        "\n",
        "# Preprocess data\n",
        "X_train = train_texts\n",
        "y_train = train_labels\n",
        "X_test = test_texts\n",
        "y_test = test_labels\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform on training data\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform validation and test data\n",
        "X_val_counts = vectorizer.transform(X_val)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(),\n",
        "    \"RandomForest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier()\n",
        "}\n",
        "\n",
        "# Initialize evaluation metrics\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score,\n",
        "    \"Precision\": precision_score,\n",
        "    \"Recall\": recall_score,\n",
        "    \"F1 Score\": f1_score\n",
        "}\n",
        "\n",
        "# Evaluate models using 10-fold cross-validation\n",
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"Evaluating {clf_name}:\")\n",
        "    cv_scores = cross_val_score(clf, X_train_counts, y_train, cv=StratifiedKFold(n_splits=10, shuffle=True), scoring='accuracy')\n",
        "    print(f\"Mean cross-validation accuracy: {np.mean(cv_scores)}\")\n",
        "\n",
        "    # Train model on entire training data\n",
        "    clf.fit(X_train_counts, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred = clf.predict(X_val_counts)\n",
        "    print(\"Validation Metrics:\")\n",
        "    for metric_name, metric_func in metrics.items():\n",
        "        print(f\"{metric_name}: {metric_func(y_val, y_pred)}\")\n",
        "\n",
        "    print(\"--------------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5d13a6-f441-4c18-c18c-34691225c1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score (K-means, Word2Vec): 0.18063719465518144\n",
            "Silhouette Score (DBSCAN, Word2Vec): 1.0\n",
            "Silhouette Score (Hierarchical, Word2Vec): 0.0017263556648420106\n",
            "Silhouette Score (Word2Vec): 0.9314850568771362\n",
            "Silhouette Score (K-means, BERT): 0.21864353120326996\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# !pip install sentence-transformers\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset (first 1000 records)\n",
        "data = pd.read_csv(\"McDonald_s_Reviews.csv\",encoding='latin-1').head(1000)\n",
        "\n",
        "# Preprocess the text data (remove punctuation, lowercase, stop words removal, tokenization)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        # Convert text to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove punctuation\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        # Remove stop words\n",
        "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text)\n",
        "        # Join tokens back to text\n",
        "        text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'Reviews' column\n",
        "data['processed_text'] = data['review'].apply(preprocess_text)\n",
        "\n",
        "# Choose appropriate features or representations\n",
        "# For K-means, DBSCAN, and Hierarchical clustering\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['processed_text'])\n",
        "\n",
        "# For Word2Vec\n",
        "word2vec_model = Word2Vec(sentences=data['processed_text'].apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.train(data['processed_text'], total_examples=len(data['processed_text']), epochs=10)\n",
        "\n",
        "# For BERT\n",
        "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "bert_embeddings = bert_model.encode(data['processed_text'])\n",
        "\n",
        "# Apply K-means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_clusters = kmeans.fit_predict(tfidf_matrix)\n",
        "kmeans_silhouette = silhouette_score(tfidf_matrix, kmeans_clusters)\n",
        "\n",
        "# Apply DBSCAN with adjusted parameters\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=10)  # Adjusted min_samples\n",
        "dbscan_clusters = dbscan.fit_predict(tfidf_matrix)\n",
        "# Filter out outliers (-1) from TF-IDF matrix\n",
        "tfidf_matrix_filtered = tfidf_matrix[dbscan_clusters != -1]\n",
        "# Filter out outliers from DBSCAN clusters\n",
        "dbscan_clusters_filtered = dbscan_clusters[dbscan_clusters != -1]\n",
        "# Calculate silhouette score using filtered data\n",
        "dbscan_silhouette = silhouette_score(tfidf_matrix_filtered, dbscan_clusters_filtered)\n",
        "\n",
        "# Adjust the sample size for hierarchical clustering due to memory constraints\n",
        "sample_size = 500  # Adjust according to your memory constraints\n",
        "hierarchical = AgglomerativeClustering(n_clusters=5)\n",
        "hierarchical_clusters = hierarchical.fit_predict(tfidf_matrix[:sample_size].toarray())\n",
        "hierarchical_silhouette = silhouette_score(tfidf_matrix[:sample_size], hierarchical_clusters)\n",
        "\n",
        "\n",
        "# Apply K-means clustering to BERT embeddings\n",
        "kmeans_bert = KMeans(n_clusters=5, random_state=42)\n",
        "bert_clusters = kmeans_bert.fit_predict(bert_embeddings)\n",
        "\n",
        "# Calculate silhouette score for BERT\n",
        "bert_silhouette = silhouette_score(bert_embeddings, bert_clusters)\n",
        "\n",
        "# Apply K-means clustering to Word2Vec vectors\n",
        "kmeans_word2vec = KMeans(n_clusters=5, random_state=42)\n",
        "word2vec_clusters = kmeans_word2vec.fit_predict(word2vec_model.wv.vectors)\n",
        "\n",
        "# Calculate silhouette score for Word2Vec\n",
        "word2vec_silhouette = silhouette_score(word2vec_model.wv.vectors, word2vec_clusters)\n",
        "\n",
        "# Output the silhouette scores including Word2Vec and BERT\n",
        "print(f\"Silhouette Score (K-means, Word2Vec): {kmeans_silhouette}\")\n",
        "print(f\"Silhouette Score (DBSCAN, Word2Vec): {dbscan_silhouette}\")\n",
        "print(f\"Silhouette Score (Hierarchical, Word2Vec): {hierarchical_silhouette}\")\n",
        "print(f\"Silhouette Score (Word2Vec): {word2vec_silhouette}\")\n",
        "print(f\"Silhouette Score (K-means, BERT): {bert_silhouette}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustering algorithms and embedding approaches performed differently while clustering text data from McDonald's reviews. K-means clustering produced a modest silhouette score, suggesting satisfactory cluster quality. DBSCAN, on the other hand, returned a flawless silhouette score, indicating that it successfully recognized dense areas in the data. However, because of its inclination to group outliers together, DBSCAN may have oversimplified the grouping. Hierarchical clustering obtained the lowest silhouette score, suggesting that the cluster quality was inadequate in comparison to other approaches. When clustered with K-means, Word2Vec embeddings had good cluster quality, however it was somewhat worse than when K-means were applied to TF-IDF features. When paired with K-means, BERT embeddings produced the greatest silhouette score of any approach, showing the efficiency.Overall, while each technique has advantages and disadvantages, BERT embeddings paired with K-means provided the most promising results for clustering McDonald's review data."
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Completing these exercises gave me practical experience in applying various text clustering techniques to real-world data.\n",
        "I learned about preprocessing, implementing different clustering algorithms, evaluating results using silhouette scores, and exploring embedding techniques like Word2Vec and BERT.\n",
        "It was valuable to see how these methods performed and to understand their strengths and limitations in clustering textual data.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}