{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhan444-s/Madhan_INFO5731_Spring2024/blob/main/Dadi_Madhan_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "'''\n",
        "Research Question: Does exposure to personalized news feeds on social media lead to less common ground and more polarized discussions during offline social interactions?\n",
        "\n",
        "Data needed are\n",
        "1.Individual data:\n",
        "a.Demographics (age, gender, location)\n",
        "b. Social media usage patterns (time spent, platform preference)\n",
        "c. News consumption preferences (topics, sources)\n",
        "d. Personality traits (openness to experience, agreeableness)\n",
        "\n",
        "\n",
        "2.Social interaction data:\n",
        "a. Recordings/transcripts of conversations during offline social gatherings\n",
        "b. Pre- and post-interaction surveys:\n",
        "c. Topics discussed\n",
        "d. Level of agreement/disagreement\n",
        "e. Perception of common ground\n",
        "f. Individual emotional state\n",
        "\n",
        "\n",
        "Amount of data:\n",
        "Ideally, data from hundreds of individuals across diverse demographics and social media usage patterns would be needed.\n",
        "For each individual, multiple recordings/transcripts of offline interactions in different settings (family, friends, colleagues) would be valuable.\n",
        "Pre- and post-interaction surveys should be conducted for each interaction.\n",
        "\n",
        "Data collection steps:\n",
        "1. Recruit participants: Advertise the study on social media and relevant online communities, targeting diverse demographics.\n",
        "2. Collect individual data: Use online surveys to gather demographic information, social media usage patterns, and personality assessments.\n",
        "3. Track news consumption: Utilize browser extensions or dedicated apps to track websites and articles participants access.\n",
        "4. Record/transcribe offline interactions: Participants wear recording devices or take detailed notes during social gatherings, later anonymized and transcribed.\n",
        "5. Administer pre- and post-interaction surveys: Participants answer questions about topics discussed, agreement/disagreement, perceived common ground, and emotional state before and after each interaction.\n",
        "\n",
        "Data saving and security:\n",
        "Store all data securely and anonymously, following ethical guidelines and data privacy regulations.\n",
        "Use encrypted databases and password protection.\n",
        "Obtain informed consent from participants regarding data collection and usage.\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import pandas as pd\n",
        "import random\n",
        "# Generate synthetic data for 1000 participants\n",
        "participants = []\n",
        "for _ in range(1000):\n",
        "    participant = {\n",
        "        'ID': _ + 1,\n",
        "        'Age': random.randint(18, 65),\n",
        "        'Gender': random.choice(['Male', 'Female', 'Other']),\n",
        "        'Location': random.choice(['Urban', 'Suburban', 'Rural']),\n",
        "        'SocialMediaUsage': random.randint(1, 10),  # hours per day\n",
        "        'PlatformPreference': random.choice(['Facebook', 'Twitter', 'Instagram']),\n",
        "        'NewsTopics': random.choice(['Politics', 'Technology', 'Health', 'Sports']),\n",
        "        'NewsSources': random.choice(['CNN', 'BBC', 'NY Times', 'BuzzFeed']),\n",
        "        'OpennessToExperience': random.uniform(1, 5),\n",
        "        'Agreeableness': random.uniform(1, 5),\n",
        "    }\n",
        "    participants.append(participant)\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df_individual = pd.DataFrame(participants)\n",
        "\n",
        "# Generate synthetic data for social interactions\n",
        "interactions = []\n",
        "for participant_id in range(1, 1001):\n",
        "    for _ in range(random.randint(2, 5)):  # random number of interactions per participant\n",
        "        interaction = {\n",
        "            'ID': participant_id,\n",
        "            'Setting': random.choice(['Family', 'Friends', 'Colleagues']),\n",
        "            'TopicsDiscussed': random.choice(['Politics', 'Technology', 'Movies', 'Food']),\n",
        "            'LevelOfAgreement': random.choice(['High', 'Medium', 'Low']),\n",
        "            'PerceivedCommonGround': random.choice(['Yes', 'No']),\n",
        "            'EmotionalStatePre': random.choice(['Happy', 'Neutral', 'Angry']),\n",
        "            'EmotionalStatePost': random.choice(['Happy', 'Neutral', 'Angry']),\n",
        "        }\n",
        "        interactions.append(interaction)\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df_interactions = pd.DataFrame(interactions)\n",
        "\n",
        "# Save data to CSV files\n",
        "df_individual.to_csv('individual_data.csv', index=False)\n",
        "df_interactions.to_csv('interaction_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "YaGLbSHHB8Ej"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def getGoogleScholarArticles(keyword, maxResults=1000):\n",
        "    baseUrl = 'https://scholar.google.com/scholar'\n",
        "    params = {\n",
        "        'q': keyword,\n",
        "        'hl': 'en',\n",
        "        'as_sdt': '0,5',\n",
        "    }\n",
        "\n",
        "    articles = []\n",
        "    count = 0\n",
        "    while count < maxResults:\n",
        "        response = requests.get(baseUrl, params=params)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for result in soup.find_all('div', class_='gs_ri'):\n",
        "            title = result.find('h3', class_='gs_rt').text\n",
        "            venue = result.find('div', class_='gs_a').text\n",
        "            authors = result.find('div', class_='gs_a').text.split('-')[0].strip()\n",
        "            #year = result.find('div', class_='gs_a').text.split('-')[-1].strip()\n",
        "# Extracting the year more reliably\n",
        "            yearMatch = re.search(r'\\b\\d{4}\\b', venue)\n",
        "            year = yearMatch.group(0) if yearMatch else 'N/A'\n",
        "\n",
        "            abstract = result.find('div', class_='gs_rs')\n",
        "            abstract = abstract.text if abstract else 'N/A'\n",
        "\n",
        "            articles.append({\n",
        "                'Title': title,\n",
        "                'Venue': venue,\n",
        "                'Authors': authors,\n",
        "                'Year': year,\n",
        "                'Abstract': abstract\n",
        "            })\n",
        "\n",
        "            count += 1\n",
        "            if count >= maxResults:\n",
        "                break\n",
        "\n",
        "        next_button = soup.find('button', class_='gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx')\n",
        "        if not next_button:\n",
        "            break\n",
        "        params['start'] = count\n",
        "\n",
        "    return articles\n",
        "\n",
        "def filterByDate(articles, start_year, end_year):\n",
        "    filteredArticles = []\n",
        "    for article in articles:\n",
        "        try:\n",
        "            year = int(article['Year'])\n",
        "            if start_year <= year <= end_year:\n",
        "                filteredArticles.append(article)\n",
        "        except ValueError:\n",
        "            # Handle non-integer values gracefully (e.g., skip the article)\n",
        "            pass\n",
        "    return filteredArticles\n",
        "\n",
        "    return [article for article in articles if start_year <= int(article['Year']) <= end_year]\n",
        "\n",
        "def main():\n",
        "    keyword = \"XYZ\"\n",
        "    maxResults = 1000\n",
        "    start_year = 2014\n",
        "    end_year = 2024\n",
        "\n",
        "    articles = getGoogleScholarArticles(keyword, maxResults)\n",
        "    filteredArticles = filterByDate(articles, start_year, end_year)\n",
        "\n",
        "    df = pd.DataFrame(filteredArticles)\n",
        "    df.to_csv('articles_data.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc82b402-5320-4d5d-f753-e025f0617155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Timestamp               Author  \\\n",
            "0  1.587424e+09               iEslam   \n",
            "1  1.594386e+09           Krukerfluk   \n",
            "2  1.588945e+09      jessjwilliamson   \n",
            "3  1.513644e+09           backprop88   \n",
            "4  1.571943e+09  janky_british_gamer   \n",
            "5  1.599933e+09           paulkaefer   \n",
            "6  1.589972e+09            Itwist101   \n",
            "7  1.594632e+09                atqm-   \n",
            "8  1.589235e+09               Nekose   \n",
            "9  1.598100e+09           HotTeenBoy   \n",
            "\n",
            "                                               Title  Score  Num_comments  \n",
            "0  Lad wrote a Python script to download Alexa vo...  12348           133  \n",
            "1                                     This post has:   9233           437  \n",
            "2  I redesign the Python logo to make it more modern   7865           266  \n",
            "3     Automate the boring stuff with python - tinder   6715           327  \n",
            "4  Just finished programming and building my own ...   6606           469  \n",
            "5  I'm excited to share my first published book, ...   6498           249  \n",
            "6  Drawing Mona Lisa with 256 circles using evolu...   5718           121  \n",
            "7  I made a simulation using Python in which a ne...   5698           212  \n",
            "8  Thanks to everyone’s advice, my mouse drawing ...   5538           203  \n",
            "9                              Debugging Cheat Sheet   5447           112  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Set up your Reddit API credentials\n",
        "redditClientId = 'czZQXwmEfuATnQ4TdsNQ_Q'\n",
        "redditClientSecret = 'dEL-Sr2eSlGDUteyjQbgcv7Q1yCZRg'\n",
        "redditUserAgent = 'MyRedditApp/1.0 by Ok_Abbreviations8589'\n",
        "\n",
        "# Authenticate with Reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id=redditClientId,\n",
        "    client_secret=redditClientSecret,\n",
        "    user_agent=redditUserAgent\n",
        ")\n",
        "\n",
        "def collect_subreddit_posts(subredditName, numPosts):\n",
        "    posts = []\n",
        "    subreddit = reddit.subreddit(subredditName)\n",
        "\n",
        "    for submission in subreddit.top(limit=numPosts):\n",
        "        posts.append([submission.created_utc, submission.author.name, submission.title, submission.score, submission.num_comments])\n",
        "\n",
        "    return posts\n",
        "\n",
        "# Example usage\n",
        "subredditToSearch = 'python'\n",
        "noOfPosts = 10\n",
        "\n",
        "collectedPosts = collect_subreddit_posts(subredditToSearch, noOfPosts)\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "columns = ['Timestamp', 'Author', 'Title', 'Score', 'Num_comments']\n",
        "df = pd.DataFrame(collectedPosts, columns=columns)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}